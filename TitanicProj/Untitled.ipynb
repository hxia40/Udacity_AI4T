{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, let’s load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "# from torchnlp.encoders import label_encoder\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the Dataset module to inherit various functions such as __getitem__(), __len__(), etc predefined in the library. These functions would help us to create our custom class for initializing the dataset. The code below shows how to create a dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self,csvpath, mode = 'train'):\n",
    "        self.mode = mode\n",
    "        df = pd.read_csv(csvpath)\n",
    "#         le = LabelEncoder()        \n",
    "#       \"\"\"       \n",
    "#         <------Some Data Preprocessing---------->\n",
    "#         Removing Null Values, Outliers and Encoding the categorical labels etc\n",
    "#       \"\"\"\n",
    "        if self.mode == 'train':\n",
    "            df = df.dropna()\n",
    "            self.inp = df.iloc[:,1:].values\n",
    "            self.oup = df.iloc[:,0].values.reshape(183,)\n",
    "        else:\n",
    "            self.inp = df.values\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            inpt  = torch.Tensor(self.inp[idx])\n",
    "            oupt  = torch.Tensor(self.oup[idx])\n",
    "            return { 'inp': inpt,\n",
    "                     'oup': oupt,\n",
    "            }\n",
    "        else:\n",
    "            inpt = torch.Tensor(self.inp[idx])\n",
    "            return { 'inp': inpt\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the above code the last column of our data frame contains the target class while rest are input features hence we split it out to self.inp and self.oup variables accordingly and we would need both inputs as well as output if we are going to train else only the input data would be needed.\n",
    "\n",
    "The __init__() function reads the .csv file using the pandas data frame and we do some preprocessing on it later (which is irrelevant to this tutorial). \n",
    "\n",
    "The __len__ ()function returns the number of examples and __getitem__() is used to fetch data by using its index. The important thing to note from the above piece of code is that we have converted our training examples into a tensor using the torch.tensor function while calling it using its index. So throughout the tutorial wherever we fetch examples it will all be in the form of tensors.\n",
    "Now since the data is ready let’s load it into batches. This can be done easily using the DataLoader function as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the DataSet\n",
    "BATCH_SIZE = 1\n",
    "data = TitanicDataset('C:/Users/hxi00/Documents/Udacity_AI4T/TitanicProj/data/train.csv')\n",
    "## Load the Dataset\n",
    "data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, shuffle =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to pass your dataset object resulting from the previous function as your argument. According to the number of batches, the result will be a multidimensional tensor of the shape (no_of_batches, batch_size, size_of_the_vector). Now, the number of dimensions would vary for other kinds of data like Image or Sequential Data accordingly based on its nature. But for now, just understand that there are multiple batches and each batch contains some examples equal to batch size (Irrespective of whatever data you use)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture\n",
    "Now since we have our data ready for training we have to design the neural network before we can start training it. Any model with conventionally used hyperparameters would be fine (Adam Optimizer, MSE Loss). To code our neural network, we can make use of the nn.Module to create the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def swish(x):\n",
    "    return x * F.sigmoid(x)\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(8, 16)\n",
    "        self.b1 = nn.BatchNorm1d(16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.b2 = nn.BatchNorm1d(8)\n",
    "        self.fc3 = nn.Linear(8,4)\n",
    "        self.b3 = nn.BatchNorm1d(4)\n",
    "        self.fc4 = nn.Linear(4,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "#         x = swish(self.fc1(x))\n",
    "#         x = self.b1(x)\n",
    "#         x = swish(self.fc2(x))\n",
    "#         x = self.b2(x)\n",
    "#         x = swish(self.fc3(x))\n",
    "#         x = self.b3(x)\n",
    "#         x = F.sigmoid(self.fc4(x))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.b1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.b2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.b3(x)\n",
    "        x = F.sigmoid(self.fc4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear(), nn.BatchNorm1d() all become available once you inherit nn.Module class(). You can then simply use them by calling it. Since we are using simple tabular data we can use a simple dense layer (or fully connected layer) to create the model. For activation, I have used swish() by a custom definition. One could go for ReLu also. ReLu is available in the nn.functional() module. You could simply replace swish() with F.relu(). Since its a binary classification it is not very necessary to use a softmax in the final layer. I have used the sigmoid function to classify my examples. In the above code, __init__() helps you to initialize your neural network model as soon as you call the constructor and forward() function controls the data flow through the network which makes it responsible for feedforward. As we proceed to the training loop you will see how we call the forward function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "Your training process can be laid as follow:\n",
    "\n",
    "You define your training parameters like no of epochs, loss function, optimizer. All the optimizers are available in torch.optim(). Your optimizer function takes the weights of the network as its parameters. In the below code net variable contains the neural network model we created in the above subsection and net.parameters() refer to the network’s weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "optimizer can only optimize Tensors, but one of the params is float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-92a8166e7802>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0moptm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[0;32m     46\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[0;32m     47\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[1;34m(self, param_group)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                 raise TypeError(\"optimizer can only optimize Tensors, \"\n\u001b[0m\u001b[0;32m    231\u001b[0m                                 \"but one of the params is \" + torch.typename(param))\n\u001b[0;32m    232\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: optimizer can only optimize Tensors, but one of the params is float"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "net_parameters = [0.5, 0.5]\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 200\n",
    "optm = Adam(net_parameters, lr = 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
