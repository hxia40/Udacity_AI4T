{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x000002557D511D00>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://medium.com/analytics-vidhya/deep-learning-tutorial-with-keras-7a34a1a322cd\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\hxi00\\Anaconda\\envs\\torch\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
      "     2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
      "    36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
      "   104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
      "  7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
      "    18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
      "    88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
      "   178   32]\n",
      " [ 163   11 3215    2    4 1153    9  194  775    7 8255    2  349 2637\n",
      "   148  605    2 8003   15  123  125   68    2 6853   15  349  165 4362\n",
      "    98    5    4  228    9   43    2 1157   15  299  120    5  120  174\n",
      "    11  220  175  136   50    9 4373  228 8255    5    2  656  245 2350\n",
      "     5    4 9837  131  152  491   18    2   32 7464 1212   14    9    6\n",
      "   371   78   22  625   64 1382    9    8  168  145   23    4 1690   15\n",
      "    16    4 1355    5   28    6   52  154  462   33   89   78  285   16\n",
      "   145   95]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hxi00\\Anaconda\\envs\\torch\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the imdb dataset\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "imdb = keras.datasets.imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#pad all input sequences to have the length of 100\n",
    "X_train = pad_sequences(x_train, maxlen=100)\n",
    "X_test = pad_sequences(x_test, maxlen=100)\n",
    "print(X_train[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the model\n",
    "#import Sequential model\n",
    "from keras import models\n",
    "#create Keras model\n",
    "model = models.Sequential()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. adding layers\n",
    "# a convolutional layer is best suited for data that contain images\n",
    "# an Embedding layer is best suited for data that contains text.\n",
    "from keras.layers import Embedding\n",
    "# first hidden layer is an embedding layer that converts each word into word vector\n",
    "model.add(Embedding(100000, 128))\n",
    "\n",
    "# Next, add the LSTM layer found in keras.layers.LSTM. LSTM layers are preferred because they can learn long-term\n",
    "# dependencies of data and thus make accurate predictions.\n",
    "# import LSTM layer\n",
    "from keras.layers import LSTM\n",
    "# second hidden layer is an LSTM layer\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Lastly, you will add a simple dense layer from keras.layers.Dense. A dense layer is a type of layer that connects\n",
    "# each input to each output within its layer.\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 142s 114ms/step - loss: 0.4259 - accuracy: 0.8057 - val_loss: 0.3545 - val_accuracy: 0.8480\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 144s 115ms/step - loss: 0.2761 - accuracy: 0.8892 - val_loss: 0.3670 - val_accuracy: 0.8450\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 143s 114ms/step - loss: 0.1993 - accuracy: 0.9236 - val_loss: 0.3788 - val_accuracy: 0.8452\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 141s 113ms/step - loss: 0.1433 - accuracy: 0.9475 - val_loss: 0.4387 - val_accuracy: 0.8359\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 140s 112ms/step - loss: 0.1092 - accuracy: 0.9613 - val_loss: 0.5687 - val_accuracy: 0.8360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2551916b970>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Compiling the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 5. Training\n",
    "# fit model on training set and check the accuracy on validation set.\n",
    "model.fit(X_train, y_train, batch_size=20, epochs=5,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
