{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from HX_DieN import DieNEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()  # Resetting the environment will return an integer. This number will be our initial state.\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "\n",
    "        # total_reward += (gamma ** step_idx * reward)\n",
    "        # the above code is from Moustafa Alzantot , which this is problematic.\n",
    "        # As the policy's target here is never to finish in shortest time. Rather,\n",
    "        # the only thing matters is that if u can successfully recover ur stuff, or drop into one of the ice-hole.\n",
    "        total_reward += reward    # HX\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    # print \"total_reward:\", total_reward\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 1000):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    scores = []\n",
    "    for _ in range(n):\n",
    "        if _ % (n/100) == 0:\n",
    "            print(\"=====sample went through =====\", _)\n",
    "        scores.append(run_episode(env, policy, gamma = gamma, render = False))\n",
    "\n",
    "\n",
    "    # # scores = [\n",
    "    # #         run_episode(env, policy, gamma = gamma, render = False)\n",
    "    # #         for _ in range(n)]\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"time consumed is\",end_time)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def run_episode_stock(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()  # Resetting the environment will return an integer. This number will be our initial state.\n",
    "    obs = 11  # that's what usually happens. for PI/VI for the daily trading, obs has to be set to 0\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        actual_obs_not_using, reward, done , _ = env.step(int(policy[obs]))\n",
    "        obs += 1\n",
    "        # print('obs:', obs, 'r:', reward, 'done:', done)\n",
    "        # time.sleep(0.3)\n",
    "\n",
    "        # total_reward += (gamma ** step_idx * reward)\n",
    "        # the above code is from Moustafa Alzantot , which this is problematic.\n",
    "        # As the policy's target here is never to finish in shortest time. Rather,\n",
    "        # the only thing matters is that if u can successfully recover ur stuff, or drop into one of the ice-hole.\n",
    "        total_reward += reward    # HX\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "\n",
    "            break\n",
    "    # print \"total_reward:\", total_reward\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy_stock(env, policy, gamma = 1.0,  n = 1000):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode_stock(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "class VI:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.V = np.zeros(env.nS)\n",
    "\n",
    "    def next_best_action(self, s, V, gamma=1):\n",
    "        # print(\"\\ns =\" , s)\n",
    "        action_values = np.zeros(self.env.nA)\n",
    "        for a in range(self.env.nA):\n",
    "            for prob, next_state, reward, done in self.env.P[s][a]:\n",
    "                # print('prob:', prob, 's_:', next_state, 'r:', reward, 'done:', done)\n",
    "                action_values[a] += prob * (reward + gamma * V[next_state])\n",
    "        return np.argmax(action_values), np.max(action_values)\n",
    "\n",
    "    def optimize(self, gamma =1):\n",
    "        # THETA = 12.3636363637\n",
    "        delta = float(\"inf\")\n",
    "        last_delta = float(\"inf\")\n",
    "        round_num = 0\n",
    "\n",
    "        while delta:\n",
    "            start_time = time.time()\n",
    "            delta = 0\n",
    "            # print(\"\\nValue Iteration: Round \" + str(round_num))\n",
    "            # print(np.reshape(self.V,(8,8)))\n",
    "            for s in range(self.env.nS):\n",
    "                best_action, best_action_value = self.next_best_action(s, self.V, gamma)\n",
    "                delta = max(delta, np.abs(best_action_value - self.V[s]))\n",
    "                self.V[s] = best_action_value\n",
    "            if last_delta/delta < 1.0000000000001:\n",
    "                break\n",
    "            else:\n",
    "                round_num += 1\n",
    "                last_delta = delta\n",
    "                print('round_num/delta/time:', round_num, delta, time.time()-start_time)\n",
    "\n",
    "        policy = np.zeros(self.env.nS)\n",
    "        for s in range(self.env.nS):\n",
    "            best_action, best_action_value = self.next_best_action(s, self.V, gamma)\n",
    "            policy[s] = best_action\n",
    "        print('VI policy:', policy)\n",
    "        print('VI table:', self.V)\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====innerloop: 22 [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0] =========\n",
      "round_num/delta/time: 1 105.181818182 0.008785247802734375\n",
      "round_num/delta/time: 2 16.8347107438 0.00725102424621582\n",
      "round_num/delta/time: 3 12.3636363636 0.010858297348022461\n",
      "VI policy: [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "VI table: [  43.27272727   50.39669421   50.21487603   50.03305785   49.85123967\n",
      "   49.66942149   49.48760331   50.27272727   51.27272727   52.27272727\n",
      "   53.27272727   54.27272727   55.27272727   56.27272727   57.27272727\n",
      "   58.27272727   59.27272727   60.27272727   61.27272727   62.27272727\n",
      "   63.27272727   64.27272727   65.27272727   66.27272727   67.27272727\n",
      "   68.27272727   69.27272727   70.27272727   71.27272727   72.27272727\n",
      "   73.27272727   74.27272727   75.27272727   76.27272727   77.27272727\n",
      "   78.27272727   79.27272727   80.27272727   81.27272727   82.27272727\n",
      "   83.27272727   84.27272727   85.27272727   86.27272727   87.27272727\n",
      "   88.27272727   89.27272727   90.27272727   91.27272727   92.27272727\n",
      "   93.27272727   94.27272727   95.27272727   96.27272727   97.27272727\n",
      "   98.27272727   99.27272727  100.27272727  101.27272727  102.27272727\n",
      "  103.27272727  104.27272727  105.27272727  106.27272727  107.27272727\n",
      "  108.27272727  109.27272727  110.27272727  111.27272727  112.27272727\n",
      "  113.27272727  114.27272727  115.27272727  116.27272727  117.27272727\n",
      "  118.27272727  119.27272727  120.27272727  121.27272727  122.27272727\n",
      "  123.27272727  124.27272727  125.27272727  126.27272727  127.27272727\n",
      "  128.27272727  129.27272727  130.27272727  131.27272727  132.27272727\n",
      "  133.27272727  134.27272727  135.27272727  136.27272727  137.27272727\n",
      "  138.27272727  139.27272727  140.27272727  141.27272727  142.27272727]\n",
      "=====sample went through ===== 0\n",
      "=====sample went through ===== 300000\n",
      "=====sample went through ===== 600000\n",
      "=====sample went through ===== 900000\n",
      "=====sample went through ===== 1200000\n",
      "=====sample went through ===== 1500000\n",
      "=====sample went through ===== 1800000\n",
      "=====sample went through ===== 2100000\n",
      "=====sample went through ===== 2400000\n",
      "=====sample went through ===== 2700000\n",
      "=====sample went through ===== 3000000\n",
      "=====sample went through ===== 3300000\n",
      "=====sample went through ===== 3600000\n",
      "=====sample went through ===== 3900000\n",
      "=====sample went through ===== 4200000\n",
      "=====sample went through ===== 4500000\n",
      "=====sample went through ===== 4800000\n",
      "=====sample went through ===== 5100000\n",
      "=====sample went through ===== 5400000\n",
      "=====sample went through ===== 5700000\n",
      "=====sample went through ===== 6000000\n",
      "=====sample went through ===== 6300000\n",
      "=====sample went through ===== 6600000\n",
      "=====sample went through ===== 6900000\n",
      "=====sample went through ===== 7200000\n",
      "=====sample went through ===== 7500000\n",
      "=====sample went through ===== 7800000\n",
      "=====sample went through ===== 8100000\n",
      "=====sample went through ===== 8400000\n",
      "=====sample went through ===== 8700000\n",
      "=====sample went through ===== 9000000\n",
      "=====sample went through ===== 9300000\n",
      "=====sample went through ===== 9600000\n",
      "=====sample went through ===== 9900000\n",
      "=====sample went through ===== 10200000\n",
      "=====sample went through ===== 10500000\n",
      "=====sample went through ===== 10800000\n",
      "=====sample went through ===== 11100000\n",
      "=====sample went through ===== 11400000\n",
      "=====sample went through ===== 11700000\n",
      "=====sample went through ===== 12000000\n",
      "=====sample went through ===== 12300000\n",
      "=====sample went through ===== 12600000\n",
      "=====sample went through ===== 12900000\n",
      "=====sample went through ===== 13200000\n",
      "=====sample went through ===== 13500000\n",
      "=====sample went through ===== 13800000\n",
      "=====sample went through ===== 14100000\n",
      "=====sample went through ===== 14400000\n",
      "=====sample went through ===== 14700000\n",
      "=====sample went through ===== 15000000\n",
      "=====sample went through ===== 15300000\n",
      "=====sample went through ===== 15600000\n",
      "=====sample went through ===== 15900000\n",
      "=====sample went through ===== 16200000\n",
      "=====sample went through ===== 16500000\n",
      "=====sample went through ===== 16800000\n",
      "=====sample went through ===== 17100000\n",
      "=====sample went through ===== 17400000\n",
      "=====sample went through ===== 17700000\n",
      "=====sample went through ===== 18000000\n",
      "=====sample went through ===== 18300000\n",
      "=====sample went through ===== 18600000\n",
      "=====sample went through ===== 18900000\n",
      "=====sample went through ===== 19200000\n",
      "=====sample went through ===== 19500000\n",
      "=====sample went through ===== 19800000\n",
      "=====sample went through ===== 20100000\n",
      "=====sample went through ===== 20400000\n",
      "=====sample went through ===== 20700000\n",
      "=====sample went through ===== 21000000\n",
      "=====sample went through ===== 21300000\n",
      "=====sample went through ===== 21600000\n",
      "=====sample went through ===== 21900000\n",
      "=====sample went through ===== 22200000\n",
      "=====sample went through ===== 22500000\n",
      "=====sample went through ===== 22800000\n",
      "=====sample went through ===== 23100000\n",
      "=====sample went through ===== 23400000\n",
      "=====sample went through ===== 23700000\n",
      "=====sample went through ===== 24000000\n",
      "=====sample went through ===== 24300000\n",
      "=====sample went through ===== 24600000\n",
      "=====sample went through ===== 24900000\n",
      "=====sample went through ===== 25200000\n",
      "=====sample went through ===== 25500000\n",
      "=====sample went through ===== 25800000\n",
      "=====sample went through ===== 26100000\n",
      "=====sample went through ===== 26400000\n",
      "=====sample went through ===== 26700000\n",
      "=====sample went through ===== 27000000\n",
      "=====sample went through ===== 27300000\n",
      "=====sample went through ===== 27600000\n",
      "=====sample went through ===== 27900000\n",
      "=====sample went through ===== 28200000\n",
      "=====sample went through ===== 28500000\n",
      "=====sample went through ===== 28800000\n",
      "=====sample went through ===== 29100000\n",
      "=====sample went through ===== 29400000\n",
      "=====sample went through ===== 29700000\n",
      "time consumed is 351.73572063446045\n",
      "Policy average score =  6.31126706667\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    '''===========DieN==========='''\n",
    "    '''obs, reward, done , _ = env.step(int(policy[obs]))'''\n",
    "    # isBadSide = [1, 1, 1, 0, 0, 0]\n",
    "    isBadSide = [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n",
    "    # isBadSide = [1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0]\n",
    "\n",
    "    # isBadSide = [0,1,1,0]   # 1.4530781083333333\n",
    "    # isBadSide = [0,0,1,0,1,0,1,1,0,1,0,0]   # 4.695342123333333\n",
    "    # isBadSide = [0,0,0,1]\n",
    "    # isBadSide = [0,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1]   # 0.94540250999999997\n",
    "    # isBadSide = [0,1,1,0,0,1,0,0,1,1,1,0,1,0,1,0,1,0,1]\n",
    "    # isBadSide = [0,1,0,1,0,1,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,1,0,1]      # 8.6673006816666671)\n",
    "    # isBadSide = [0,0,1,1,1,0,0,0,1,0]\n",
    "    # isBadSide = [0,0,1,0,1,0,1,1,0]\n",
    "    # isBadSide = [0,0,1,0,1,0,1,1,0,1,1,1,1,1,0,1,0]\n",
    "    # isBadSide = [0,0,1,1,0,0,0,0,1,0,1,1,1,0,1,0,1,1]\n",
    "\n",
    "#     isBadSide = [0,0,1,1,0,0,0,0,1,0,1,1,1,0,1,0,1,1]\n",
    "    # isBadSide = [0,0,0,1,1,0,0,0,0,0,1,0,1,0]\n",
    "    # isBadSide = [0,0,1,1,0,1,1]\n",
    "    # isBadSide = [0,0,0,0,1,1,1,1,1,1,0,0,0,0,1]\n",
    "    # isBadSide = [0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,1,1,1,1]\n",
    "    # isBadSide = [0,1,1,0,0,1,1]\n",
    "    # isBadSide = [0,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,1,1,0]\n",
    "    # isBadSide = [0,1]\n",
    "    # isBadSide = [0,1,0,0,0,0,1,1,1,0,1,0,0,1,1,0,0,0,1,0,1,0,0,0,0,1,0,1]\n",
    "    # isBadSide = [0,0,1,0,1,0,1,1,0]\n",
    "\n",
    "    n = len(isBadSide)\n",
    "    slip=0\n",
    "    env_DN = DieNEnv(n=n, isBadSide=isBadSide, slip=slip)\n",
    "    env_DN.slip = 0\n",
    "\n",
    "    # print(env_DN.nA)\n",
    "    # print(env_DN.nS)\n",
    "    # print(env_DN.isBadSide)\n",
    "    # print(\"==========================\")\n",
    "    # print(env_DN.P)\n",
    "    # print(\"==========================\")\n",
    "    vi_DN = VI(env_DN)\n",
    "    optimal_policy_DN = vi_DN.optimize(gamma=1)\n",
    "\n",
    "    policy_score = evaluate_policy(env_DN, optimal_policy_DN,\n",
    "                                   # n = 1000\n",
    "                                   n=60000000\n",
    "                                   )\n",
    "    print('Policy average score = ', policy_score)\n",
    "    # '''===========Frozenlake==========='''\n",
    "    # env_name  = 'FrozenLake8x8-v0'\n",
    "    # env = gym.make(env_name)\n",
    "    # vi = VI(env)\n",
    "    # optimal_policy = vi.optimize(gamma=1)\n",
    "    # policy_score = evaluate_policy(env, optimal_policy, n=1)\n",
    "    # print('Policy average score = ', policy_score)\n",
    "    # print('end')\n",
    "    # '''===========stocks==========='''\n",
    "    # '''obs, reward, done , _ = env.step(int(policy[obs]))'''\n",
    "    #\n",
    "    # env_AT = StocksEnv(df=pd.read_csv('IBM.csv'),frame_bound=(50, 100), window_size=10)\n",
    "    # print(env_AT.nA)\n",
    "    # print(env_AT.nS)\n",
    "    # print(\"==========================\")\n",
    "    # print(env_AT.P)\n",
    "    # print(\"==========================\")\n",
    "    # vi_AT = VI(env_AT)\n",
    "    # optimal_policy_AT = vi_AT.optimize(gamma=1)\n",
    "    # policy_score = evaluate_policy_stock(env_AT, optimal_policy_AT, n=1000)\n",
    "    # print('Policy average score = ', policy_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
